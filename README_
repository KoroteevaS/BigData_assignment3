
First part: Instruction to install Spark cluster on local machine : 

 1- install Java 8 or latest from link below
   https://www.oracle.com/java/technologies/downloads/#java8

 2-Then, download Apache Spark as zip. 
   https://spark.apache.org/downloads.html

 3- Next, unzip downloaded file and copy the underlying 
  folder ( spark-3.0.0-bin-hadoop2.7 ) into local program folder.

 4- Now, set the environment variable in local machine, as below: 
      SPARK_HOME  = C:\apps\spark-3.0.0-bin-hadoop2.7
      HADOOP_HOME = C:\apps\spark-3.0.0-bin-hadoop2.7
      PATH=%PATH%;C:\apps\spark-3.0.0-bin-hadoop2.7\bin
      Install pyspark: python -m pip install pyspark==3.0 (your pyspark version should be equal to your spark-bin package)

 5- Finally, open CMD or PowerShell and type pyspark command to run the pyspark shell. The command will show the screen below.


Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.4.0
      /_/

    Using Python version 3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021 13:44:55)
    Spark context Web UI available at http://T111A-L8hthJ5qD.mshome.net:4040
    Spark context available as 'sc' (master = local[*], app id = local-1686309046105).
    SparkSession available as 'spark'.

-------------------------------------------------------------------------------------------------

Second Part: Instructions for server cluster:

1- Connect  to server with ssh.
2- Connect cluster: ssh 246a-1
3- Prepare setup file (setup.csh) with your environmental variables:
	export HADOOP_VERSION=2.8.0
	export HADOOP_PREFIX=/local/Hadoop/hadoop-$HADOOP_VERSION
	PATH=${PATH}:$HADOOP_PREFIX/bin
	export SPARK_HOME=/home/username/spark-3.2.3-bin-hadoop2.7
	PATH=$SPARK_HOME/bin:$PATH
	export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
	export YARN_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
	need java8

	Note: path to your spark-bin package could be different.
 4- Install pyspark: python -m pip install pyspark==3.2.3 (your package version)
 5- Run command: source setup.csh
 6- Put “kdd.data” dataset to hadoop cluster space: 
 hdfs dfs -put kdd.data /user/username/
 7- For logging change paths to csv file twice in the script inserting your username into the path.
 8- Run python files: “spark-submit –master yarn -–deploy-mode cluster ass3_group_dt_v1.py" and "“spark-submit –master yarn -–deploy-mode cluster ass3_group_lr_v1.py"
 9- Access log folders dt_output_fin.csv, dt_output.csv folders for decision tree scripts and lr_output.csv, lr_output_fin.csv 
 for logistic regression output via:
 hdfs dfs -ls /user/username/dt_output_fin.csv 
 and copy file started with part to you space  via :
 hdfs dfs -get /user/username/dt_output_fin.csv/part......scv /home/username
 10- Overall statistic in folders ending with "_output_fin.csv". Progress in folders "_output.csv"
